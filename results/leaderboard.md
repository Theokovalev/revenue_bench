# Revenue Bench Leaderboard

*Last Updated: 2025-08-13*

## Full Rankings (33 Models Evaluated)

| Rank | Model | Provider | Score | Cost/Eval | Perf/$ | Status |
|------|-------|----------|-------|-----------|--------|--------|
| 🥇 **1** | **Claude Opus 4.1** | Anthropic | **82.5%** | $2.09 | 0.39 | ✅ |
| 🥈 **2** | **GPT-5** | OpenAI | **82.1%** | $0.08 | 10.24 | ✅ |
| 🥉 **3** | **Gemini 2.5 Flash** | Google | **80.0%** | $0.08 | 9.49 | ✅ |
| 4 | Claude Opus 4 | Anthropic | 78.8% | $0.98 | 0.80 | ✅ |
| 5 | GPT-OSS-120B | OpenAI | 78.7% | $0.004 | **203.91** | ✅ |
| 6 | GLM-4.5 | Z-AI | 77.9% | $0.12 | 6.44 | ✅ |
| 7 | Grok-4 | X-AI | 77.5% | $0.31 | 2.49 | ✅ |
| 8 | Claude Sonnet 4 | Anthropic | 75.8% | $0.47 | 1.61 | ✅ |
| 9 | Kimi-K2 | Moonshot | 74.6% | $0.09 | 7.91 | ✅ |
| 10 | O3 | OpenAI | 73.3% | $0.08 | 9.68 | ✅ |
| 11 | Grok-3 | X-AI | 72.5% | $0.34 | 2.11 | ✅ |
| 12 | GLM-4-32B | Z-AI | 72.1% | $0.07 | 10.45 | ✅ |
| 13 | GPT-4.1 | OpenAI | 67.9% | $0.07 | 9.06 | ✅ |
| 14 | GPT-5-Mini | OpenAI | 64.2% | $0.07 | 8.94 | ✅ |
| 15 | Gemini 2.5 Flash Lite | Google | 62.9% | $0.07 | 9.58 | ✅ |
| 16 | O3-Mini | OpenAI | 60.0% | $0.07 | 8.65 | ✅ |
| 17 | Qwen3-235B-Thinking | Qwen | 54.6% | $0.07 | 8.19 | ✅ |
| 18 | DeepSeek-R1 | DeepSeek | 52.1% | $0.08 | 6.85 | ✅ |
| 19 | Qwen3-30B-Instruct | Qwen | 51.3% | $0.06 | 8.34 | ✅ |
| 20 | Sonar-Pro | Perplexity | 47.1% | $0.08 | 5.68 | ✅ |
| 21 | Qwen-2.5-72B | Qwen | 46.2% | $0.07 | 6.88 | ✅ |
| 22 | Gemma-3-27B | Google | 45.8% | $0.07 | 6.22 | ✅ |
| 23 | Jamba-Large-1.7 | AI21 | 45.4% | $0.07 | 6.59 | ✅ |
| 24 | Mixtral-8x22B | Mistral | 43.8% | $0.06 | 6.78 | ✅ |
| 25 | Llama-3.1-70B | Meta | 43.3% | $0.07 | 5.81 | ✅ |
| 26 | Mixtral-8x7B | Mistral | 42.9% | $0.06 | 6.64 | ✅ |
| 27 | Llama-4-Maverick | Meta | 40.8% | $0.06 | 6.69 | ✅ |
| 28 | Jamba-Mini-1.7 | AI21 | 30.8% | $0.07 | 4.40 | ✅ |
| 29 | GPT-OSS-20B | OpenAI | 2.5% | $0.04 | 0.57 | ⚠️ |
| 30 | Gemini-2.5-Pro | Google | 2.5% | $0.08 | 0.31 | ⚠️ |
| 31 | GPT-5-Nano | OpenAI | 0.0% | $0.05 | 0.00 | ❌ |
| 32 | Grok-3-Mini | X-AI | 0.0% | $0.05 | 0.00 | ❌ |
| 33 | GLM-4.5-Air:Free | Z-AI | 0.0% | $0.05 | 0.00 | ❌ |

## Key Insights

### 🏆 Winners by Category

**Best Overall Performance**
- 🥇 Claude Opus 4.1 (82.5%)
- 🥈 GPT-5 (82.1%)
- 🥉 Gemini 2.5 Flash (80.0%)

**Best Value (Performance per Dollar)**
- 🥇 GPT-OSS-120B (203.91)
- 🥈 GLM-4-32B (10.45)
- 🥉 GPT-5 (10.24)

**Most Cost-Effective (Under $0.10)**
- GPT-OSS-120B: 78.7% at $0.004
- GPT-5: 82.1% at $0.08
- Gemini 2.5 Flash: 80.0% at $0.08

### 📊 Performance Tiers

**Elite Tier (>75%)**
- 8 models
- Average cost: $0.54
- Best value: GPT-OSS-120B

**Professional Tier (60-75%)**
- 8 models
- Average cost: $0.15
- Best value: GLM-4-32B

**Competent Tier (40-60%)**
- 11 models
- Average cost: $0.07
- Best value: Qwen3-30B-Instruct

**Struggling Tier (<40%)**
- 6 models
- Average cost: $0.06
- Not recommended for production

### 💰 Cost Analysis

- **Total benchmark cost**: $6.11
- **Average cost per model**: $0.19
- **Most expensive**: Claude Opus 4.1 ($2.09)
- **Least expensive**: GPT-OSS-120B ($0.004)

### 🔍 Notable Findings

1. **Open-source models compete well**: GPT-OSS-120B achieves 78.7% at minimal cost
2. **Price doesn't guarantee performance**: Gemini-2.5-Pro costs $0.08 but scores only 2.5%
3. **Tool use is critical**: Models scoring <40% typically failed to use web search tools
4. **Sweet spot**: Models around $0.07-0.08 offer best balance of performance and cost

## Evaluation Criteria

Models were scored on:
- **Engineering Pain Recognition** (35%): Identifying operational challenges
- **Prospect-Specific Insight** (30%): Using verified, non-obvious information
- **Product-Market Fit** (25%): Natural connection to value proposition
- **Reply Probability** (10%): Likelihood of generating a response

## Methodology

- **Task**: Personalized cold outreach for Homebase (3 prospects)
- **Judges**: 4 AI models (Gemini 2.5 Pro, Kimi K2, GPT-5 Mini, Claude Opus 4.1)
- **Scoring**: Median across judges, average across prospects
- **Verification**: Claims must be backed by evidence URLs

## Status Legend

- ✅ **Success**: Completed task with valid output
- ⚠️ **Partial**: Generated output but severe quality issues
- ❌ **Failed**: No valid output or critical errors

---

*For detailed methodology, see [METHODOLOGY.md](../docs/METHODOLOGY.md)*
*To add your model, see [ADD_MODEL.md](../docs/ADD_MODEL.md)*